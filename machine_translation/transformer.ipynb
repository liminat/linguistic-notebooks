
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Translation with Transformer\n",
    "\n",
    "In this notebook, we will show how to use Transformer introduced in [1] and evaluate the pretrained model with GluonNLP. Transformer model is shown to be more accurate and easier to parallelize than previous seq2seq-based models such as Google Neural Machine Translation. We will use the state-of-the-art pretrained Transformer model, evaluate the pretrained Transformer model on newstest2014 and translate a few sentences ourselves with the `BeamSearchTranslator`;\n",
    "\n",
    "## Preparation\n",
    "\n",
    "We start with some usual preparation such as importing libraries and setting the environment.\n",
    "\n",
    "### Load MXNet and GluonNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: en_core_web_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.1.0/en_core_web_sm-2.1.0.tar.gz#egg=en_core_web_sm==2.1.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (2.1.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/en_core_web_sm\n",
      "-->\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/spacy/data/en\n",
      "You can now load the model via spacy.load('en')\n",
      "Requirement already satisfied: de_core_news_sm==2.1.0 from https://github.com/explosion/spacy-models/releases/download/de_core_news_sm-2.1.0/de_core_news_sm-2.1.0.tar.gz#egg=de_core_news_sm==2.1.0 in /home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages (2.1.0)\n",
      "\u001b[33mYou are using pip version 10.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('de_core_news_sm')\n",
      "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/de_core_news_sm\n",
      "-->\n",
      "/home/ec2-user/anaconda3/envs/mxnet_p36/lib/python3.6/site-packages/spacy/data/de\n",
      "You can now load the model via spacy.load('de')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en\n",
    "!python -m spacy download de\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import gluonnlp as nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "ctx = mx.gpu(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use the Pretrained Transformer model\n",
    "\n",
    "Next, we load the Transformer model in GluonNLP model zoo and use the full newstest2014 segment of WMT 2014 English-German test dataset, and evaluate the model on it.\n",
    "\n",
    "### Get the Transformer\n",
    "\n",
    "We load the pretrained Transformer using the model API in GluonNLP, which returns the source and target vocabulary along with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36794 36794\n"
     ]
    }
   ],
   "source": [
    "import nmt\n",
    "\n",
    "wmt_model_name = 'transformer_en_de_512'\n",
    "\n",
    "wmt_transformer_model, wmt_src_vocab, wmt_tgt_vocab = \\\n",
    "    nlp.model.get_model(wmt_model_name,\n",
    "                        dataset_name='WMT2014',\n",
    "                        pretrained=True,\n",
    "                        ctx=ctx)\n",
    "\n",
    "# we are using mixed vocab of EN-DE, so the source and target language vocab are the same\n",
    "print(len(wmt_src_vocab), len(wmt_tgt_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Transformer model architecture is shown as below:\n",
    "\n",
    "<div style=\"width: 500px;\">![transformer](transformer.png)</div>\n",
    "\n",
    "### Load and Preprocess WMT 2014 Dataset\n",
    "\n",
    "We then load the newstest2014 segment in WMT 2014 English-German test dataset for evaluation purpose.\n",
    "\n",
    "The following shows how to process the dataset and cache the processed dataset\n",
    "for the future use. The processing steps include:\n",
    "\n",
    "1) clip the source and target sequences\n",
    "2) split the string input to a list of tokens\n",
    "3) map the string token into its index in the vocabulary\n",
    "4) append EOS token to source sentence and add BOS and EOS tokens to target sentence.\n",
    "\n",
    "Let's first look at the WMT 2014 corpus. GluonNLP provides [WMT2014BPE](../../api/modules/data.rst#gluonnlp.data.WMT2014BPE)\n",
    "and [WMT2014](../../api/modules/data.rst#gluonnlp.data.WMT2014) classes. The former contains BPE-tokenized dataset, while\n",
    "the later contains the raw text. Here, we use the former for scoring, and the later for\n",
    "demonstrating actual translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source language en, Target language de\n",
      "Sample BPE tokens: \"('Or@@ land@@ o Blo@@ om and Mir@@ anda Ker@@ r still love each other', 'Or@@ land@@ o Blo@@ om und Mir@@ anda Ker@@ r lieben sich noch immer')\"\n",
      "Sample raw text: \"('Orlando Bloom and Miranda Kerr still love each other', 'Orlando Bloom und Miranda Kerr lieben sich noch immer')\"\n",
      "Sample target sentence: \"Orlando Bloom und Miranda Kerr lieben sich noch immer\"\n"
     ]
    }
   ],
   "source": [
    "import hyperparameters as hparams\n",
    "\n",
    "wmt_data_test = nlp.data.WMT2014BPE('newstest2014',\n",
    "                                    src_lang=hparams.src_lang,\n",
    "                                    tgt_lang=hparams.tgt_lang)\n",
    "print('Source language %s, Target language %s' % (hparams.src_lang, hparams.tgt_lang))\n",
    "print('Sample BPE tokens: \"{}\"'.format(wmt_data_test[0]))\n",
    "\n",
    "wmt_test_text = nlp.data.WMT2014('newstest2014',\n",
    "                                 src_lang=hparams.src_lang,\n",
    "                                 tgt_lang=hparams.tgt_lang)\n",
    "print('Sample raw text: \"{}\"'.format(wmt_test_text[0]))\n",
    "\n",
    "wmt_test_tgt_sentences = wmt_test_text.transform(lambda src, tgt: tgt)\n",
    "print('Sample target sentence: \"{}\"'.format(wmt_test_tgt_sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transform the machine translation dataset.\n",
      "\n",
      "    Clip source and the target sentences to the maximum length. For the source sentence, append the\n",
      "    EOS. For the target sentence, append BOS and EOS.\n",
      "\n",
      "    Parameters\n",
      "    ----------\n",
      "    src_vocab : Vocab\n",
      "    tgt_vocab : Vocab\n",
      "    src_max_len : int\n",
      "    tgt_max_len : int\n",
      "    \n",
      "[ 7300 21964 23833  1935 24004 11836  6698 11839  5565 25464 27950 22544\n",
      " 16202 24272     3]\n",
      "[    2  7300 21964 23833  1935 24004 29615  6698 11839  5565 25464 22297\n",
      " 27121 23712 20558     3]\n"
     ]
    }
   ],
   "source": [
    "import dataprocessor\n",
    "\n",
    "print(dataprocessor.TrainValDataTransform.__doc__)\n",
    "\n",
    "# wmt_transform_fn includes the four preprocessing steps mentioned above.\n",
    "wmt_transform_fn = dataprocessor.TrainValDataTransform(wmt_src_vocab, wmt_tgt_vocab)\n",
    "wmt_dataset_processed = wmt_data_test.transform(wmt_transform_fn, lazy=False)\n",
    "print(*wmt_dataset_processed[0], sep='\\n')\n",
    "\n",
    "def get_length_index_fn():\n",
    "    global idx\n",
    "    idx = 0\n",
    "    def transform(src, tgt):\n",
    "        global idx\n",
    "        result = (src, tgt, len(src), len(tgt), idx)\n",
    "        idx += 1\n",
    "        return result\n",
    "    return transform\n",
    "\n",
    "wmt_data_test_with_len = wmt_dataset_processed.transform(get_length_index_fn(), lazy=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Sampler and DataLoader for WMT 2014 Dataset\n",
    "\n",
    "Now, we have obtained the transformed datasets. The next step is to construct sampler and DataLoader. First, we need to construct batchify function, which pads and stacks sequences to form mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmt_test_batchify_fn = nlp.data.batchify.Tuple(\n",
    "    nlp.data.batchify.Pad(),\n",
    "    nlp.data.batchify.Pad(),\n",
    "    nlp.data.batchify.Stack(dtype='float32'),\n",
    "    nlp.data.batchify.Stack(dtype='float32'),\n",
    "    nlp.data.batchify.Stack())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In GluonNLP, all dataset items are tuples. In the preprocessed `wmt_data_test_with_len`, it includes\n",
    "`(src, tgt, len(src), len(tgt), idx)` elements. In order to express how we'd like to batchify them\n",
    "in GluonNLP, we use the built-in batchify functions.\n",
    "\n",
    "* [Tuple](../../api/modules/data.batchify.rst#gluonnlp.data.batchify.Tuple) is the GluonNLP way of applying different batchify functions to each element of a dataset item. In this case, we are applying `Pad` to `src` and `tgt`, `Stack` to `len(src)` and `len(tgt)` with conversion to float32, and simple `Stack` to `idx` without type conversion.\n",
    "* [Pad](../../api/modules/data.batchify.rst#gluonnlp.data.batchify.Pad) takes the elements from all dataset items in a batch, and pad them according to the item of maximum length to form a padded matrix/tensor.\n",
    "* [Stack](../../api/modules/data.batchify.rst#gluonnlp.data.batchify.Stack) simply stacks all elements in a batch, and requires all elements to be of the same length.\n",
    "\n",
    "\n",
    "We can then construct bucketing samplers, which generate batches by grouping sequences with similar lengths. Here, we use [FixedBucketSampler](../../api/modules/data.rst#gluonnlp.data.FixedBucketSampler) with [ExpWidthBucket](../../api/modules/data.rst#gluonnlp.data.ExpWidthBucket). FixedBucketSampler aims to assign each data sample to a fixed bucket based on its length. With this setting, the sampler would select buckets following an approximately exponentially increasing interval of maximum bucket lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FixedBucketSampler:\n",
      "  sample_num=2737, batch_num=364\n",
      "  key=[10, 14, 19, 25, 33, 42, 53, 66, 81, 100]\n",
      "  cnt=[101, 243, 386, 484, 570, 451, 280, 172, 41, 9]\n",
      "  batch_size=[25, 18, 13, 10, 8, 6, 5, 4, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "wmt_bucket_scheme = nlp.data.ExpWidthBucket(bucket_len_step=1.2)\n",
    "wmt_test_batch_sampler = nlp.data.FixedBucketSampler(\n",
    "    lengths=wmt_data_test_with_len.transform(lambda src, tgt, src_len, tgt_len, idx: tgt_len), # target length\n",
    "    use_average_length=True, # control the element lengths (i.e. number of tokens) to be about the same\n",
    "    bucket_scheme=wmt_bucket_scheme,\n",
    "    batch_size=256)\n",
    "print(wmt_test_batch_sampler.stats())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the samplers, we can use [DataLoader](https://mxnet.apache.org/versions/master/api/python/gluon/data.html#mxnet.gluon.data.DataLoader) to sample the datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "364"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wmt_test_data_loader = gluon.data.DataLoader(\n",
    "    wmt_data_test_with_len,\n",
    "    batch_sampler=wmt_test_batch_sampler,\n",
    "    batchify_fn=wmt_test_batchify_fn,\n",
    "    num_workers=8)\n",
    "len(wmt_test_data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [