{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Neural Machine Translation\n",
    "\n",
    "In this notebook, we are going to train Google NMT on IWSLT 2015 English-Vietnamese\n",
    "Dataset. The building process includes four steps: 1) load and process dataset, 2)\n",
    "create sampler and DataLoader, 3) build model, and 4) write training epochs.\n",
    "\n",
    "## Load MXNET and Gluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import gluonnlp as nlp\n",
    "import nmt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Logs will be saved to gnmt_en_vi_u512/<ipython-input-2-4699ac3a1bfb>.log\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'gnmt_en_vi_u512'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(100)\n",
    "random.seed(100)\n",
    "mx.random.seed(10000)\n",
    "ctx = mx.gpu(0)\n",
    "\n",
    "# parameters for dataset\n",
    "dataset = 'IWSLT2015'\n",
    "src_lang, tgt_lang = 'en', 'vi'\n",
    "src_max_len, tgt_max_len = 50, 50\n",
    "\n",
    "# parameters for model\n",
    "num_hidden = 512\n",
    "num_layers = 2\n",
    "num_bi_layers = 1\n",
    "dropout = 0.2\n",
    "\n",
    "# parameters for training\n",
    "batch_size, test_batch_size = 128, 32\n",
    "num_buckets = 5\n",
    "epochs = 1\n",
    "clip = 5\n",
    "lr = 0.001\n",
    "lr_update_factor = 0.5\n",
    "log_interval = 10\n",
    "save_dir = 'gnmt_en_vi_u512'\n",
    "\n",
    "#parameters for testing\n",
    "beam_size = 10\n",
    "lp_alpha = 1.0\n",
    "lp_k = 5\n",
    "\n",
    "nmt.utils.logging_config(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Preprocess Dataset\n",
    "\n",
    "The following shows how to process the dataset and cache the processed dataset\n",
    "for future use. The processing steps include: 1) clip the source and target\n",
    "sequences, 2) split the string input to a list of tokens, 3) map the\n",
    "string token into its integer index in the vocabulary, and 4) append end-of-sentence (EOS) token to source\n",
    "sentence and add BOS and EOS tokens to target sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/ec2-user/.mxnet/datasets/iwslt2015/iwslt15.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/iwslt2015/iwslt15.zip...\n",
      "Processing time spent: 5.671424865722656\n",
      "Processing time spent: 0.062300920486450195\n",
      "Processing time spent: 0.057126522064208984\n"
     ]
    }
   ],
   "source": [
    "def cache_dataset(dataset, prefix):\n",
    "    \"\"\"Cache the processed npy dataset  the dataset into a npz\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : gluon.data.SimpleDataset\n",
    "    file_path : str\n",
    "    \"\"\"\n",
    "    if not os.path.exists(nmt._constants.CACHE_PATH):\n",
    "        os.makedirs(nmt._constants.CACHE_PATH)\n",
    "    src_data = np.concatenate([e[0] for e in dataset])\n",
    "    tgt_data = np.concatenate([e[1] for e in dataset])\n",
    "    src_cumlen = np.cumsum([0]+[len(e[0]) for e in dataset])\n",
    "    tgt_cumlen = np.cumsum([0]+[len(e[1]) for e in dataset])\n",
    "    np.savez(os.path.join(nmt._constants.CACHE_PATH, prefix + '.npz'),\n",
    "             src_data=src_data, tgt_data=tgt_data,\n",
    "             src_cumlen=src_cumlen, tgt_cumlen=tgt_cumlen)\n",
    "\n",
    "\n",
    "def load_cached_dataset(prefix):\n",
    "    cached_file_path = os.path.join(nmt._constants.CACHE_PATH, prefix + '.npz')\n",
    "    if os.path.exists(cached_file_path):\n",
    "        print('Load cached data from {}'.format(cached_file_path))\n",
    "        npz_data = np.load(cached_file_path)\n",
    "        src_data, tgt_data, src_cumlen, tgt_cumlen = [npz_data[n] for n in\n",
    "                ['src_data', 'tgt_data', 'src_cumlen', 'tgt_cumlen']]\n",
    "        src_data = np.array([src_data[low:high] for low, high in zip(src_cumlen[:-1], src_cumlen[1:])])\n",
    "        tgt_data = np.array([tgt_data[low:high] for low, high in zip(tgt_cumlen[:-1], tgt_cumlen[1:])])\n",
    "        return gluon.data.ArrayDataset(np.array(src_data), np.array(tgt_data))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "class TrainValDataTransform(object):\n",
    "    \"\"\"Transform the machine translation dataset.\n",
    "\n",
    "    Clip source and the target sentences to the maximum length. For the source sentence, append the\n",
    "    EOS. For the target sentence, append BOS and EOS.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    src_vocab : Vocab\n",
    "    tgt_vocab : Vocab\n",
    "    src_max_len : int\n",
    "    tgt_max_len : int\n",
    "    \"\"\"\n",
    "    def __init__(self, src_vocab, tgt_vocab, src_max_len, tgt_max_len):\n",
    "        self._src_vocab = src_vocab\n",
    "        self._tgt_vocab = tgt_vocab\n",
    "        self._src_max_len = src_max_len\n",
    "        self._tgt_max_len = tgt_max_len\n",
    "\n",
    "    def __call__(self, src, tgt):\n",
    "        if self._src_max_len > 0:\n",
    "            src_sentence = self._src_vocab[src.split()[:self._src_max_len]]\n",
    "        else:\n",
    "            src_sentence = self._src_vocab[src.split()]\n",
    "        if self._tgt_max_len > 0:\n",
    "            tgt_sentence = self._tgt_vocab[tgt.split()[:self._tgt_max_len]]\n",
    "        else:\n",
    "            tgt_sentence = self._tgt_vocab[tgt.split()]\n",
    "        src_sentence.append(self._src_vocab[self._src_vocab.eos_token])\n",
    "        tgt_sentence.insert(0, self._tgt_vocab[self._tgt_vocab.bos_token])\n",
    "        tgt_sentence.append(self._tgt_vocab[self._tgt_vocab.eos_token])\n",
    "        src_npy = np.array(src_sentence, dtype=np.int32)\n",
    "        tgt_npy = np.array(tgt_sentence, dtype=np.int32)\n",
    "        return src_npy, tgt_npy\n",
    "\n",
    "\n",
    "def process_dataset(dataset, src_vocab, tgt_vocab, src_max_len=-1, tgt_max_len=-1):\n",
    "    start = time.time()\n",
    "    dataset_processed = dataset.transform(TrainValDataTransform(src_vocab, tgt_vocab,\n",
    "                                                                src_max_len,\n",
    "                                                                tgt_max_len), lazy=False)\n",
    "    end = time.time()\n",
    "    print('Processing time spent: {}'.format(end - start))\n",
    "    return dataset_processed\n",
    "\n",
    "\n",
    "def load_translation_data(dataset, src_lang='en', tgt_lang='vi'):\n",
    "    \"\"\"Load translation dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : str\n",
    "    src_lang : str, default 'en'\n",
    "    tgt_lang : str, default 'vi'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    data_train_processed : Dataset\n",
    "        The preprocessed training sentence pairs\n",
    "    data_val_processed : Dataset\n",
    "        The preprocessed validation sentence pairs\n",
    "    data_test_processed : Dataset\n",
    "        The preprocessed test sentence pairs\n",
    "    val_tgt_sentences : list\n",
    "        The target sentences in the validation set\n",
    "    test_tgt_sentences : list\n",
    "        The target sentences in the test set\n",
    "    src_vocab : Vocab\n",
    "        Vocabulary of the source language\n",
    "    tgt_vocab : Vocab\n",
    "        Vocabulary of the target language\n",
    "    \"\"\"\n",
    "    common_prefix = 'IWSLT2015_{}_{}_{}_{}'.format(src_lang, tgt_lang,\n",
    "                                                   src_max_len, tgt_max_len)\n",
    "    data_train = nlp.data.IWSLT2015('train', src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "    data_val = nlp.data.IWSLT2015('val', src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "    data_test = nlp.data.IWSLT2015('test', src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "    src_vocab, tgt_vocab = data_train.src_vocab, data_train.tgt_vocab\n",
    "    data_train_processed = load_cached_dataset(common_prefix + '_train')\n",
    "    if not data_train_processed:\n",
    "        data_train_processed = process_dataset(data_train, src_vocab, tgt_vocab,\n",
    "                                               src_max_len, tgt_max_len)\n",
    "        cache_dataset(data_train_processed, common_prefix + '_train')\n",
    "    data_val_processed = load_cached_dataset(common_prefix + '_val')\n",
    "    if not data_val_processed:\n",
    "        data_val_processed = process_dataset(data_val, src_vocab, tgt_vocab)\n",
    "        cache_dataset(data_val_processed, common_prefix + '_val')\n",
    "    data_test_processed = load_cached_dataset(common_prefix + '_test')\n",
    "    if not data_test_processed:\n",
    "        data_test_processed = process_dataset(data_test, src_vocab, tgt_vocab)\n",
    "        cache_dataset(data_test_processed, common_prefix + '_test')\n",
    "    fetch_tgt_sentence = lambda src, tgt: tgt.split()\n",
    "    val_tgt_sentences = list(data_val.transform(fetch_tgt_sentence))\n",
    "    test_tgt_sentences = list(data_test.transform(fetch_tgt_sentence))\n",
    "    return data_train_processed, data_val_processed, data_test_processed, \\\n",
    "           val_tgt_sentences, test_tgt_sentences, src_vocab, tgt_vocab\n",
    "\n",
    "\n",
    "def get_data_lengths(dataset):\n",
    "    return list(dataset.transform(lambda srg, tgt: (len(srg), len(tgt))))\n",
    "\n",
    "\n",
    "data_train, data_val, data_test, val_tgt_sentences, test_tgt_sentences, src_vocab, tgt_vocab\\\n",
    "    = load_translation_data(dataset=dataset, src_lang=src_lang, tgt_lang=tgt_lang)\n",
    "data_train_lengths = get_data_lengths(data_train)\n",
    "data_val_lengths = get_data_lengths(data_val)\n",
    "data_test_lengths = get_data_lengths(data_test)\n",
    "\n",
    "with io.open(os.path.join(save_dir, 'val_gt.txt'), 'w', encoding='utf-8') as of:\n",
    "    for ele in val_tgt_sentences:\n",
    "        of.write(' '.join(ele) + '\\n')\n",
    "\n",
    "with io.open(os.path.join(save_dir, 'test_gt.txt'), 'w', encoding='utf-8') as of:\n",
    "    for ele in test_tgt_sentences:\n",
    "        of.write(' '.join(ele) + '\\n')\n",
    "\n",
    "\n",
    "data_train = data_train.transform(lambda src, tgt: (src, tgt, len(src), len(tgt)), lazy=False)\n",
    "data_val = gluon.data.SimpleDataset([(ele[0], ele[1], len(ele[0]), len(ele[1]), i)\n",
    "                                     for i, ele in enumerate(data_val)])\n",
    "data_test = gluon.data.SimpleDataset([(ele[0], ele[1], len(ele[0]), len(ele[1]), i)\n",
    "                                      for i, ele in enumerate(data_test)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Sampler and DataLoader\n",
    "\n",
    "Now, we have obtained `data_train`, `data_val`, and `data_test`. The next step\n",
    "is to construct sampler and DataLoader. The first step is to construct batchify\n",
    "function, which pads and stacks sequences to form mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(),\n",
    "                                            nlp.data.batchify.Pad(),\n",
    "                                            nlp.data.batchify.Stack(dtype='float32'),\n",
    "                                            nlp.data.batchify.Stack(dtype='float32'))\n",
    "test_batchify_fn = nlp.data.batchify.Tuple(nlp.data.batchify.Pad(),\n",
    "                                           nlp.data.batchify.Pad(),\n",
    "                                           nlp.data.batchify.Stack(dtype='float32'),\n",
    "                                           nlp.data.batchify.Stack(dtype='float32'),\n",
    "                                           nlp.data.batchify.Stack())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then construct bucketing samplers, which generate batches by grouping\n",
    "sequences with similar lengths. Here, the bucketing scheme is empirically determined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-03 04:58:41,300 - root - Train Batch Sampler:\n",
      "FixedBucketSampler:\n",
      "  sample_num=133166, batch_num=1043\n",
      "  key=[(9, 10), (16, 17), (26, 27), (37, 38), (51, 52)]\n",
      "  cnt=[11414, 34897, 37760, 23480, 25615]\n",
      "  batch_size=[128, 128, 128, 128, 128]\n",
      "2019-07-03 04:58:41,304 - root - Valid Batch Sampler:\n",
      "FixedBucketSampler:\n",
      "  sample_num=1553, batch_num=52\n",
      "  key=[(22, 28), (40, 52), (58, 76), (76, 100), (94, 124)]\n",
      "  cnt=[1037, 432, 67, 10, 7]\n",
      "  batch_size=[32, 32, 32, 32, 32]\n",
      "2019-07-03 04:58:41,307 - root - Test Batch Sampler:\n",
      "FixedBucketSampler:\n",
      "  sample_num=1268, batch_num=42\n",
      "  key=[(23, 29), (43, 53), (63, 77), (83, 101), (103, 125)]\n",
      "  cnt=[770, 381, 84, 26, 7]\n",
      "  batch_size=[32, 32, 32, 32, 32]\n"
     ]
    }
   ],
   "source": [
    "bucket_scheme = nlp.data.ExpWidthBucket(bucket_len_step=1.2)\n",
    "train_batch_sampler = nlp.data.FixedBucketSampler(lengths=data_train_lengths,\n",
    "                                                  batch_size=batch_size,\n",
    "                                                  num_buckets=num_buckets,\n",
    "                                                  shuffle=True,\n",
    "                                                  bucket_scheme=bucket_scheme)\n",
    "logging.info('Train Batch Sampler:\\n{}'.format(train_batch_sampler.stats()))\n",
    "val_batch_sampler = nlp.data.FixedBucketSampler(lengths=data_val_lengths,\n",
    "                                                batch_size=test_batch_size,\n",
    "                                                num_buckets=num_buckets,\n",
    "                                                shuffle=False)\n",
    "logging.info('Valid Batch Sampler:\\n{}'.format(val_batch_sampler.stats()))\n",
    "test_batch_sampler = nlp.data.FixedBucketSampler(lengths=data_test_lengths,\n",
    "                                                 batch_size=test_batch_size,\n",
    "                                                 num_buckets=num_buckets,\n",
    "                                                 shuffle=False)\n",
    "logging.info('Test Batch Sampler:\\n{}'.format(test_batch_sampler.stats()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the samplers, we can create DataLoader, which is iterable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_loader = gluon.data.DataLoader(data_train,\n",
    "                                          batch_sampler=train_batch_sampler,\n",
    "                                          batchify_fn=train_batchify_fn,\n",
    "                                          num_workers=4)\n",
    "val_data_loader = gluon.data.DataLoader(data_val,\n",
    "                                        batch_sampler=val_batch_sampler,\n",
    "                                        batchify_fn=test_batchify_fn,\n",
    "                                        num_workers=4)\n",
    "test_data_loader = gluon.data.DataLoader(data_test,\n",
    "                                         batch_sampler=test_batch_sampler,\n",
    "                                         batchify_fn=test_batchify_fn,\n",
    "                                         num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build GNMT Model\n",
    "\n",
    "After obtaining DataLoader, we can build the model. The GNMT encoder and decoder\n",
    "can be easily constructed by calling `get_gnmt_encoder_decoder` function. Then, we\n",
    "feed the encoder and decoder to `NMTModel` to construct the GNMT model.\n",
    "`model.hybridize` allows computation to be done using the symbolic backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-07-03 04:58:52,816 - root - NMTModel(\n",
      "  (encoder): GNMTEncoder(\n",
      "    (dropout_layer): Dropout(p = 0.2, axes=())\n",
      "    (rnn_cells): HybridSequential(\n",
      "      (0): BidirectionalCell(forward=LSTMCell(None -> 2048), backward=LSTMCell(None -> 2048))\n",
      "      (1): LSTMCell(None -> 2048)\n",
      "    )\n",
      "  )\n",
      "  (decoder): GNMTDecoder(\n",
      "    (attention_cell): DotProductAttentionCell(\n",
      "      (_dropout_layer): Dropout(p = 0.0, axes=())\n",
      "      (_proj_query): Dense(None -> 512, linear)\n",
      "    )\n",
      "    (dropout_layer): Drop