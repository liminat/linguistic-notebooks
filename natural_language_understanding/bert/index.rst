Bidirectional Encoder Representations from Transformers
-------------------------------------------------------

:download:`Download scripts </model_zoo/bert.zip>`

Reference: Devlin, Jacob, et al. "`Bert: Pre-training of deep bidirectional transformers for language understanding. <https://arxiv.org/abs/1810.04805>`_" arXiv preprint arXiv:1810.04805 (2018).

Note: BERT model requires `nightly version of MXNet <https://mxnet.incubator.apache.org/versions/master/install/index.html?version=master&platform=Linux&language=Python&processor=CPU>`__. 

The following pre-trained BERT models are available from the **gluonnlp.model.get_model** API:

+-------------------------------+----------------+-----------------+
|                               | bert_12_768_12 | bert_24_1024_16 |
+===============================+================+=================+
| book_corpus_wiki_en_uncased   | ✓              | ✓               |
+-------------------------------+----------------+-----------------+
| book_corpus_wiki_en_cased     | ✓              | ✓               |
+-------------------------------+----------------+-----------------+
| wiki_multilingual_uncased     | ✓              | x               |
+-------------------------------+----------------+-----------------+
| wiki_multilingual_cased       | ✓              | x               |
+-------------------------------+----------------+-----------------+
| wiki_cn_cased                 | ✓              | x               |
+-------------------------------+----------------+-----------------+
| scibert_scivocab_uncased      | ✓              | x               |
+-------------------------------+----------------+-----------------+
| scibert_scivocab_cased        | ✓              | x               |
+-------------------------------+----------------+-----------------+
| scibert_basevocab_uncased     | ✓              | x               |
+-------------------------------+----------------+-----------------+
| scibert_basevocab_cased       | ✓              | x               |
+-------------------------------+----------------+-----------------+
| biobert_v1.0_pmc_cased        | ✓              | x               |
+-------------------------------+----------------+-----------------+
| biobert_v1.0_pubmed_cased     | ✓              | x               |
+-------------------------------+----------------+-----------------+
| biobert_v1.0_pubmed_pmc_cased | ✓              | x               |
+-------------------------------+----------------+-----------------+
| biobert_v1.1_pubmed_cased     | ✓              | x               |
+-------------------------------+----------------+-----------------+
| clinicalbert_uncased          | ✓              | x               |
+-------------------------------+----------------+-----------------+

where **bert_12_768_12** refers to the BERT BASE model, and **bert_24_1024_16** refers to the BERT LARGE model.

BERT for Sentence Classification
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

GluonNLP provides the following example script to fine-tune sentence classification with pre-trained
BERT model.

.. editing URL for the following table: https://tinyurl.com/y4n8q84w

+---------------------+--------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------+
| Dataset             | MRPC                                                                                                         | RTE                                                                                                         | SST-2                                                                                                       | MNLI-M/MM                                                                                                    | XNLI (Chinese)                                                                                               |
+=====================+==============================================================================================================+=============================================================================================================+=============================================================================================================+==============================================================================================================+==============================================================================================================+
| Validation Accuracy | 88.7%                                                                                                        | 70.8%                                                                                                       | 93%                                                                                                         | 84.55%, 84.66%                                                                                               | 78.27%                                                                                                       |
+---------------------+----