Bidirectional Encoder Representations from Transformers
-------------------------------------------------------

:download:`Download scripts </model_zoo/bert.zip>`

Reference: Devlin, Jacob, et al. "`Bert: Pre-training of deep bidirectional transformers for language understanding. <https://arxiv.org/abs/1810.04805>`_" arXiv preprint arXiv:1810.04805 (2018).

Note: BERT model requires `nightly version of MXNet <https://mxnet.incubator.apache.org/versions/master/install/index.html?version=master&platform=Linux&language=Python&processor=CPU>`__. 

The following pre-trained BERT models are available from the **gluonnlp.model.get_model** API:

+-------------------------------+----------------+-----------------+
|                               | bert_12_768_12 | bert_24_1024_16 |
+===============================+================+=================+
| book_corpus_wiki_en_uncased   | ✓              | ✓               |
+-------------------------------+----------------+------------