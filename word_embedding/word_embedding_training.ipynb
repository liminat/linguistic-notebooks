{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import itertools\n",
    "import time\n",
    "import math\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import mxnet as mx\n",
    "import gluonnlp as nlp\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# context = mx.cpu()  # Enable this to run on CPU\n",
    "context = mx.gpu(0)  # Enable this to run on GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "Here we use the Text8 corpus from the [Large Text Compression\n",
    "Benchmark](http://mattmahoney.net/dc/textdata.html) which includes the first\n",
    "100\n",
    "MB of cleaned text from the English Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /var/lib/jenkins/workspace/gluon-nlp-gpu-py3@2/tests/data/datasets/text8/text8-6c70299b.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/large_text_compression_benchmark/text8-6c70299b.zip...\n",
      "# sentences: 1701\n",
      "# tokens: 10000 ['anarchism', 'originated', 'as', 'a', 'term']\n",
      "# tokens: 10000 ['reciprocity', 'qualitative', 'impairments', 'in', 'communication']\n",
      "# tokens: 10000 ['with', 'the', 'aegis', 'of', 'zeus']\n"
     ]
    }
   ],
   "source": [
    "text8 = nlp.data.Text8()\n",
    "print('# sentences:', len(text8))\n",
    "for sentence in text8[:3]:\n",
    "    print('# tokens:', len(sentence), sentence[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the tokenized data, we first count all tokens and then construct a\n",
    "vocabulary of all tokens that occur at least 5 times in the dataset. The\n",
    "vocabulary contains a one-to-one mapping between tokens and integers (also\n",
    "called indices or short idx).\n",
    "\n",
    "We further store the frequency count of each\n",
    "token in the vocabulary as we will require this information later on for\n",
    "sampling random negative (or noise) words. Finally we replace all tokens with\n",
    "their integer representation based on the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# sentences: 1701\n",
      "# tokens: 9895 [5233, 3083, 11, 5, 194]\n",
      "# tokens: 9858 [18214, 17356, 36672, 4, 1753]\n",
      "# tokens: 9926 [23, 0, 19754, 1, 4829]\n"
     ]
    }
   ],
   "source": [
    "counter = nlp.data.count_tokens(itertools.chain.from_iterable(text8))\n",
    "vocab = nlp.Vocab(counter, unknown_token=None, padding_token=None,\n",
    "                  bos_token=None, eos_token=None, min_freq=5)\n",
    "idx_to_counts = [counter[w] for w in vocab.idx_to_token]\n",
    "\n",
    "def code(sentence):\n",
    "    return [vocab[token] for token in sentence if token in vocab]\n",
    "\n",
    "text8 = text8.transform(code, lazy=False)\n",
    "\n",
    "print('# sentences:', len(text8))\n",
    "for sentence in text8[:3]:\n",
    "    print('# tokens:', len(sentence), sentence[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to transform the coded Text8 dataset into batches useful for\n",
    "training an embedding model.\n",
    "In this tutorial we train with the SkipGram\n",
    "objective made popular by [1].\n",
    "\n",
    "For SkipGram, we sample pairs of co-occurring\n",
    "words from the corpus.\n",
    "Two words are said to co-occur if they occur with\n",
    "distance less than a specified *window* size.\n",
    "The *window* size is usually\n",
    "chosen around 5.\n",
    "\n",
    "For obtaining the samples from the corpus, we can shuffle the\n",
    "sentences and the proceed linearly through each sentence, considering each word\n",
    "as well as all the words in it's window. In this case, we call the current word\n",
    "in focus the center word, and the words in it's window the context words.\n",
    "GluonNLP contains `gluonnlp.data.EmbeddingCenterContextBatchify` batchify\n",
    "transformation, that takes a corpus, such as the coded Text8 we have here, and\n",
    "returns a `DataStream` of batches of center and context words.\n",
    "\n",
    "\n",
    "\n",
    "To obtain good\n",
    "results, each sentence is further subsampled, meaning that words are deleted\n",
    "with a probability proportional to their frequency.\n",
    "[1] proposes to discard\n",
    "individual occurrences of words from the dataset with probability\n",
    "\n",
    "$$P(w_i) = 1 -\n",
    "\\sqrt{\\frac{t}{f(w_i)}}$$\n",
    "\n",
    "where $f(w_i)$ is the frequency with which a word is\n",
    "observed in a dataset and $t$ is a subsampling constant typically chosen around\n",
    "$10^{-5}$.\n",
    "[1] has also shown that the final performance is improved if the\n",
    "window size is chosen  uniformly random for each center words out of the range\n",
    "[1, *window*].\n",
    "\n",
    "For this notebook, we are interested in training a fastText\n",
    "embedding model [2]. A fastText model not only associates a embedding vector to\n",
    "each token in the vocabulary, but also to a pre-specified number of subwords.\n",
    "Commonly 2 million subword vectors are obtained and each subword vector is\n",
    "associated with zero, one or multiple character-ngrams. The mapping between\n",
    "character-ngrams and subwords is based on a hash function.\n",
    "The *final* embedding\n",
    "vector of a token is the mean of the vectors associated with the token and all\n",
    "character-ngrams occurring in the string representation of the token. Thereby a\n",
    "fastText embedding model can compute meaningful embedding vectors for tokens\n",
    "that were not seen during training.\n",
    "\n",
    "For this notebook, we have prepared a helper function `transform_data_fasttext`\n",
    "which builds a series of transformations of the `text8` `Dataset` created above,\n",
    "applying \"tricks\" mentioned before. It returns a `DataStream` over batches as\n",
    "well as a batchify_fn function that applied to a batch looks up and includes the\n",
    "fastText subwords associated with the center words and finally the subword\n",
    "function that can be used to obtain the subwords of a given string\n",
    "representation of a token. We will take a closer look at the subword function\n",
    "shortly.\n",
    "\n",
    "Note that the number of subwords is potentially\n",
    "