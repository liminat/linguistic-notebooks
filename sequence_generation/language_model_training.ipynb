{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM-based Language Models\n",
    "\n",
    "A statistical language model is simply a probability distribution over sequences of words or characters [1].\n",
    "In this tutorial, we'll restrict our attention to word-based language models.\n",
    "Given a reliable language model, we can answer questions like *which among the following strings are we more likely to encounter?*\n",
    "\n",
    "1. 'On Monday, Mr. Lamar’s “DAMN.” took home an even more elusive honor,\n",
    "one that may never have even seemed within reach: the Pulitzer Prize\"\n",
    "1. \"Frog zealot flagged xylophone the bean wallaby anaphylaxis extraneous\n",
    "porpoise into deleterious carrot banana apricot.\"\n",
    "\n",
    "Even if we've never seen either of these sentences in our entire lives, and even though no rapper has previously been\n",
    "awarded a Pulitzer Prize, we wouldn't be shocked to see the first sentence in the New York Times.\n",
    "By comparison, we can all agree that the second sentence, consisting of incoherent babble, is comparatively unlikely.\n",
    "A statistical language model can assign precise probabilities to each string of words.\n",
    "\n",
    "Given a large corpus of text, we can estimate (i.e. train) a language model $\\hat{p}(x_1, ..., x_n)$.\n",
    "And given such a model, we can sample strings $\\mathbf{x} \\sim \\hat{p}(x_1, ..., x_n)$, generating new strings according to their estimated probability.\n",
    "Among other useful applications, we can use language models to score candidate transcriptions from speech recognition models, given a preference to sentences that seem more probable (at the expense of those deemed anomalous).\n",
    "\n",
    "These days recurrent neural networks (RNNs) are the preferred method for LM. In this notebook, we will go through an example of using GluonNLP to\n",
    "\n",
    "(i) implement a typical LSTM language model architecture\n",
    "(ii) train the language model on a corpus of real data\n",
    "(iii) bring in your own dataset for training\n",
    "(iv) grab off-the-shelf pre-trained state-of-the-art language models (i.e., AWD language model) using GluonNLP.\n",
    "\n",
    "## Language model definition - one sentence\n",
    "\n",
    "The standard approach to language modeling consists of training a model that given a trailing window of text, predicts the next word in the sequence.\n",
    "When we train the model we feed in the inputs $x_1, x_2, ...$ and try at each time step to predict the corresponding next word $x_2, ..., x_{n+1}$.\n",
    "To generate text from a language model, we can iteratively predict the next word, and then feed this word as an input to the model at the subsequent time step.\n",
    "\n",
    "<img src=\"https://gluon.mxnet.io/_images/recurrent-lm.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "## Train your own language model\n",
    "\n",
    "Now let's step through how to train your own\n",
    "language model using GluonNLP.\n",
    "\n",
    "\n",
    "### Preparation\n",
    "\n",
    "We'll start by taking care of\n",
    "our basic dependencies and setting up our environment\n",
    "\n",
    "#### Load gluonnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m nltk.downloader all\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import glob\n",
    "import time\n",
    "import math\n",
    "\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, autograd\n",
    "from mxnet.gluon.utils import download\n",
    "\n",
    "import gluonnlp as nlp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_gpus = 1\n",
    "context = [mx.gpu(i) for i in range(num_gpus)] if num_gpus else [mx.cpu()]\n",
    "log_interval = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 20 * len(context)\n",
    "lr = 20\n",
    "epochs = 3\n",
    "bptt = 35\n",
    "grad_clip = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load dataset, extract vocabulary, numericalize, and batchify for truncated Back Propagation Through Time (BPTT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading /home/ec2-user/.mxnet/datasets/wikitext-2/wikitext-2-v1.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/wikitext-2/wikitext-2-v1.zip...\n"
     ]
    }
   ],
   "source": [
    "dataset_name = 'wikitext-2'\n",
    "train_dataset, val_dataset, test_dataset = [\n",
    "    nlp.data.WikiText2(\n",
    "        segment=segment, bos=None, eos='<eos>', skip_empty=False)\n",
    "    for segment in ['train', 'val', 'test']\n",
    "]\n",
    "\n",
    "vocab = nlp.Vocab(\n",
    "    nlp.data.Counter(train_dataset), padding_token=None, bos_token=None)\n",
    "\n",
    "bptt_batchify = nlp.data.batchify.CorpusBPTTBatchify(\n",
    "    vocab, bptt, batch_size, last_batch='discard')\n",
    "train_data, val_data, test_data = [\n",
    "    bptt_batchify(x) for x in [train_dataset, val_dataset, test_dataset]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-defined language model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StandardRNN(\n",
      "  (embedding): HybridSequential(\n",
      "    (0): Embedding(33278 -> 200, float32)\n",
      "    (1): Dropout(p = 0.2, axes=())\n",
      "  )\n",
      "  (encoder): LSTM(200 -> 200, TNC, num_layers=2, dropout=0.2)\n",
      "  (decoder): HybridSequential(\n",
      "    (0): Dense(200 -> 33278, linear)\n",
      "  )\n",
      ")\n",
      "Vocab(size=33278, unk=\"<unk>\", reserved=\"['<eos>']\")\n"
     ]
    }
   ],
   "source": [
    "model_name = 'standard_lstm_lm_200'\n",
    "model, vocab = nlp.model.get_model(model_name, vocab=vocab, dataset_name=None)\n",
    "print(model)\n",
    "print(vocab)\n",
    "\n",
    "model.initialize(mx.init.Xavier(), ctx=context)\n",
    "\n",
    "trainer = gluon.Trainer(model.collect_params(), 'sgd', {\n",
    "    'learning_rate': lr,\n",
    "    'momentum': 0,\n",
    "    'wd': 0\n",
    "})\n",
    "loss = gluon.loss.SoftmaxCrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now that everything is ready, we can start training the model.\n",
    "\n",
    "#### Detach gradients on states for truncated BPTT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detach(hidden):\n",
    "    if isinstance(hidden, (tuple, list)):\n",
    "        hidden = [detach(i) for i in hidden]\n",
    "    else:\n",
    "        hidden = hidden.detach()\n",
    "    return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_source, batch_size, ctx):\n",
    "    total_L = 0.0\n",
    "    ntotal = 0\n",
    "    hidden = model.begin_state(\n",
    "        batch_size=batch_size, func=mx.nd.zeros, ctx=ctx)\n",
    "    for i, (data, target) in enumerate(data_source):\n",
    "        data = data.as_in_context(ctx)\n",
    "        target = target.as_in_context(ctx)\n",
    "        output, hidden = model(data, hidden)\n",
    "        hidden = detach(hidden)\n",
    "        L = loss(output.reshape(-3, -1), target.reshape(-1))\n",
    "        total_L += mx.nd.sum(L).asscalar()\n",
    "        ntotal += L.size\n",
    "    return total_L / ntotal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training loop\n",
    "\n",
    "Our loss function will be the standard cross-entropy loss function used for multiclass classification, applied at each time step to compare our predictions to the true next word in the sequence.\n",
    "We can calculate gradients with respect to our parameters using truncated [back-propagation-through-time (BPTT)](https://en.wikipedia.org/wiki/Backpropagation_through_time).\n",
    "In this case, we'll backpropagate for $35$ time steps, updating our weights with stochastic gradient descent with the learning rate of $20$, hyperparameters that we chose earlier in the notebook.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/ee/Unfold_through_time.png\" width=\"500\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, test_data, epochs, lr):\n",
    "    best_val = float(\"Inf\")\n",
    "    start_train_time = time.time()\n",
    "    parameters = model.collect_params().values()\n",
    "    for epoch in range(epochs):\n",
    "        total_L = 0.0\n",
    "        start_epoch_time = time.time()\n",
    "        start_log_interval_time = time.time()\n",
    "        hiddens = [model.begin_state(batch_size//len(context), func=mx.nd.zeros, ctx=ctx)\n",
    "                   for ctx in context]\n",
    "        for i, (data, target) in enumerate(train_data):\n",
    "            data_list = gluon.utils.split_and_load(data, context,\n",
    "                                                   batch_axis=1, even_split=True)\n",
    "            target_list = gluon.utils.split_and_load(target, context,\n",
    "                                                     batch_axis=1, even_split=True)\n",
    "            hiddens = detach(hiddens)\n",
    "            L = 0\n",
    "            Ls = []\n",
    "            with autograd.record():\n",
    "                for j, (X, y, h) in enumerate(zip(data_list, target_list, hiddens)):\n",
    "                    output, h = model(X, h)\n",
    "                    batch_L = loss(output.reshape(-3, -1), y.reshape(-1,))\n",
    "                    L = L + batch_L.as_in_context(context[0]) / (len(context) * X.size)\n",
    "                    Ls.append(batch_L / (len(context) * X.size))\n",
    "                    hiddens[j] = h\n",
    "            L.backward()\n",
    "            grads = [p.grad(x.context) for p in parameters for x in data_list]\n",
    "            gluon.utils.clip_global_norm(grads, grad_clip)\n",
    "\n",
    "            trainer.step(1)\n",
    "\n",
    "            total_L += sum([mx.nd.sum(l).asscalar() for l in Ls])\n",
    "\n",
    "            if i % log_interval == 0 and i > 0:\n",
    "                cur_L = total_L / log_interval\n",
    "                print('[Epoch %d Batch %d/%d] loss %.2f, ppl %.2f, '\n",
    "                      'throughput %.2f samples/s'%(\n",
    "                    epoch, i, len(train_data), cur_L, math.exp(cur_L),\n",
    "                    batch_size * log_interval / (time.time() - start_log_interval_time)))\n",
    "                total_L = 0.0\n",
    "                start_log_interval_time = time.time()\n",
    "\n",
    "        mx.nd.waitall()\n",
    "\n",
    "        print('[Epoch %d] throughput %.2f samples/s'%(\n",
    "                    epoch, len(train_data)*batch_size / (time.time() - start_epoch_time)))\n",
    "        val_L = evaluate(model, val_data, batch_size, context[0])\n",
    "        print('[Epoch %d] time cost %.2fs, valid loss %.2f, valid ppl %.2f'%(\n",
    "            epoch, time.time()-start_epoch_time, val_L, math.exp(val_L)))\n",
    "\n",
    "        if val_L < best_val:\n",
    "            best_val = val_L\n",
    "            test_L = evaluate(model, test_data, batch_size, context[0])\n",
    "            model.save_parameters('{}_{}-{}.params'.format(model_name, dataset_name, epoch))\n",
    "            print('test loss %.2f, test ppl %.2f'%(test_L, math.exp(test_L)))\n",
    "        else:\n",
    "            lr = lr*0.25\n",
    "            print('Learning rate now %f'%(lr))\n",
    "            trainer.set_learning_rate(lr)\n",
    "\n",
    "    print('Total training throughput %.2f samples/s'%(\n",
    "                            (batch_size * len(train_data) * epochs) /\n",
    "                            (time.time() - start_train_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0 Batch 200/2983] loss 7.66, ppl 2122.19, throughput 519.53 samples/s\n",
      "[Epoch 0 Batch 400/2983] loss 6.77, ppl 870.92, throughput 525.18 samples/s\n",
      "[Epoch 0 Batch 600/2983] loss 6.36, ppl 578.28, throughput 503.41 samples/s\n",
      "[Epoch 0 Batch 800/2983] loss 6.19, ppl 485.60, throughput 526.85 samples/s\n",
      "[Epoch 0 Batch 1000/2983] loss 6.05, ppl 425.62, throughput 502.80 samples/s\n",
      "[Epoch 0 Batch 1200/2983] loss 5.98, ppl 395.73, throughput 528.83 samples/s\n",
      "[Epoch 0 Batch 1400/2983] loss 5.87, ppl 355.99, throughput 503.50 samples/s\n",
      "[Epoch 0 Batch 1600/2983] loss 5.88, ppl 359.07, throughput 524.95 samples/s\n",
      "[Epoch 0 Batch 1800/2983] loss 5.72, ppl 306.14, throughput 524.21 samples/s\n",
      "[Epoch 0 Batch 2000/2983] loss 5.69, ppl 294.49, throughput 502.67 samples/s\n",
      "[Epoch 0 Batch 2200/2983] loss 5.58, ppl 264.13, throughput 525.69 samples/s\n",
      "[Epoch 0 Batch 2400/2983] loss 5.58, ppl 265.85, throughput 504.15 samples/s\n",
      "[Epoch 0 Batch 2600/2983] loss 5.57, ppl 263.18, throughput 525.96 samples/s\n",
      "[Epoch 0 Batch 2800/2983] loss 5.46, ppl 235.26, throughput 503.94 samples/s\n",
      "[Epoch 0] throughput 516.64 samples/s\n",
      "[Epoch 0] time cost 127.66s, valid loss 5.41, valid ppl 224.52\n",
      "test loss 5.33, test ppl 206.45\n",
      "[Epoch 1 Batch 200/2983] loss 5.47, ppl 237.43, throughput 523.98 samples/s\n",
      "[Epoch 1 Batch 400/2983] loss 5.45, ppl 232.42, throughput 503.03 